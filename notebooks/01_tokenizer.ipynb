{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6211df26",
   "metadata": {},
   "source": [
    "# Tokenizer Training\n",
    "\n",
    "## Objective\n",
    "\n",
    "Following the data preparation in the previous notebook, this notebook focuses on the next critical step: **training a tokenizer**. Using the clean, filtered training corpus we generated, we will train a custom Byte-Pair Encoding (BPE) tokenizer.\n",
    "\n",
    "The final artifact produced by this notebook is a trained tokenizer (vocabulary and merge rules) saved to disk. This tokenizer will be used in all subsequent stages to convert raw text into token IDs that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b7020",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b774baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mini_transformer.data.tokenize.bpe import BPETokenization\n",
    "from mini_transformer.container import MiniTransformerContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ca297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = MiniTransformerContainer()\n",
    "container.init_resources()\n",
    "container.wire(modules=[__name__])\n",
    "repo = container.data.repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d3df0",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, we initialize the project's dependency container to gain access to our services, specifically the `DatasetRepository`.\n",
    "\n",
    "We then query the repository to find and load the **filtered training dataset** that was created in the `00_data.ipynb` notebook. This ensures we are training the tokenizer on the exact same clean data that will be used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329ef98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = repo.show(stage=\"filtered\", split=\"train\")\n",
    "dataset = repo.get(dataset_id=dataset_info['id'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667850b",
   "metadata": {},
   "source": [
    "## 2. Train and Save the Tokenizer\n",
    "\n",
    "With the training corpus loaded, we can now perform the core task of this notebook:\n",
    "\n",
    "1.  **Instantiate the Tokenizer Service**: We get the `BPETokenization` service from our container.\n",
    "2.  **Train**: We call the `.train()` method, passing our dataset. This process iterates through the text corpus to learn the subword vocabulary and merge rules that define our BPE tokenizer.\n",
    "3.  **Save**: Once training is complete, we call the `.save()` method. This serializes the trained tokenizer's state (vocabulary, etc.) to files, making it a reusable artifact for model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba5088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE tokenizer training on wmt14-fr-en-train-filtered-50000_examples-62438e4e started.\n",
      "\n",
      "\n",
      "\n",
      "BPE tokenizer training complete.\n"
     ]
    }
   ],
   "source": [
    "tokenization = container.data.tokenization()\n",
    "tokenization.train(dataset=dataset)\n",
    "tokenization.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minix4mr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
