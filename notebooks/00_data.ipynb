{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ddcf48f",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook details the end-to-end data preparation pipeline for the WMT14 French-to-English translation dataset. The primary goal is to process the raw source data into clean, filtered, and appropriately sized datasets for training, validation, and testing a transformer model.\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "The workflow is executed in two primary stages:\n",
    "\n",
    "1.  **Raw Data Extraction**: We begin by extracting a large number of examples from the source dataset for each split (4M for training, 3k for validation, and 3k for testing). This initial step ensures we have a comprehensive base of raw data stored and managed by our dataset repository.\n",
    "\n",
    "2.  **Filtering & Subsampling**: The raw datasets are then passed through a filtering process. This stage is crucial for quality control and creating a manageable dataset for training. It involves cleaning the data based on certain criteria and significantly subsampling the training set from 4 million down to 50,000 examples, resulting in a high-quality dataset ready for the next steps in the modeling pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a91e66",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mini_transformer.data.builder.data_filter import TranslationDatasetBuilderFilteredConfig, TranslationDatasetBuilderFiltered\n",
    "from mini_transformer.data.builder.extractor import TranslationDatasetBuilderRaw, TranslationDatasetBuilderRawConfig\n",
    "from mini_transformer.container import MiniTransformerContainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c23f7f",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization\n",
    "\n",
    "We begin by setting up the environment. This involves initializing a central `MiniTransformerContainer` which handles dependency injection for the project. By wiring this container, we gain access to a `DatasetRepository`, which is a dedicated service for managing, storing, and retrieving our datasets throughout their various processing stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e91a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = MiniTransformerContainer()\n",
    "container.init_resources()\n",
    "container.wire(modules=[__name__])\n",
    "repo = container.data.repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4386f9",
   "metadata": {},
   "source": [
    "## 2. Stage 1: Raw Data Extraction\n",
    "\n",
    "The first step in our data pipeline is to extract the raw data from the source, the WMT14 French-to-English dataset. We define the desired number of samples for each split:\n",
    "\n",
    "* **Training**: 4,000,000 examples\n",
    "* **Validation**: 3,000 examples\n",
    "* **Test**: 3,000 examples\n",
    "\n",
    "The `extract` function is a helper that encapsulates this logic. It checks if a dataset with the specified configuration already exists in our repository. If not, it uses the `TranslationDatasetBuilderRaw` to build it from the source and add it to the repository. The output from the subsequent cell confirms that these raw datasets have already been created and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15a2489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(split, n, stage=\"raw\"):\n",
    "    config = TranslationDatasetBuilderRawConfig(split=split, stage=stage, n=n, seed=55)\n",
    "    if not repo.exists(config.dataset_id):\n",
    "        extractor = TranslationDatasetBuilderRaw(config=config)\n",
    "        dataset = extractor.build()\n",
    "        repo.add(dataset=dataset)\n",
    "        print(dataset.metrics)\n",
    "    else:\n",
    "        print(f\"Dataset {config.dataset_name} already exists.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f5fc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wmt14-fr-en-train-raw-4000000_examples-80e3e0a4 already exists.\n",
      "Dataset wmt14-fr-en-validation-raw-3000_examples-adb3cda6 already exists.\n",
      "Dataset wmt14-fr-en-test-raw-3000_examples-019a7ef1 already exists.\n"
     ]
    }
   ],
   "source": [
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "sizes = [4000000, 3000, 3000]\n",
    "stages = [\"raw\", \"raw\", \"raw\"]\n",
    "for split, n, stage in zip(splits, sizes, stages):\n",
    "    extract(split=split, n=n, stage=stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9405b15",
   "metadata": {},
   "source": [
    "## 3. Stage 2: Filtering and Processing\n",
    "\n",
    "With the raw datasets available, the next stage is to apply filtering and cleaning. This is a critical step to create a high-quality, manageable dataset for training. The `dataset_filter` function handles this process. It retrieves a raw dataset, creates a new configuration for a \"filtered\" version, and then uses the `TranslationDatasetBuilderFiltered` to perform the actual processing.\n",
    "\n",
    "The final code cell iterates through all available \"raw\" datasets and applies this filtering logic to each one. The output shows the details of the newly created filtered datasets. It's important to note the change in size:\n",
    "\n",
    "* The training set is subsampled from **4,000,000** to **50,000** examples.\n",
    "* The validation and test sets are slightly reduced, likely due to filtering out pairs that didn't meet specific criteria (e.g., sentence length, character ratios, etc.).\n",
    "\n",
    "This results in a smaller, cleaner dataset that is more efficient for model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5767bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_filter(dataset_id: str, force: bool = False):\n",
    "    dataset = repo.get(dataset_id=dataset_id)\n",
    "    config = TranslationDatasetBuilderFilteredConfig.from_config(dataset.config)\n",
    "    if force:\n",
    "        repo.remove(config.dataset_id)\n",
    "    if not repo.exists(config.dataset_id):\n",
    "        filter = TranslationDatasetBuilderFiltered(dataset=dataset, config=config)\n",
    "        filtered_dataset = filter.build()\n",
    "        repo.add(filtered_dataset)\n",
    "        print(filtered_dataset)\n",
    "    else:\n",
    "        print(f\"Dataset {config.dataset_name} already exists.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3cde5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11561.98it/s]\n",
      "[08/27/2025 01:00:48 PM] [INFO] [mini_transformer.data.repo] [add] : Added dataset id: 62438e4e, name: wmt14-fr-en-train-filtered-50000_examples-62438e4e to the Dataset repository.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                       TranslationDataset                       \n",
      "                              id | 62438e4e\n",
      "                            name | wmt14-fr-en-train-filtered-50000_examples-62438e4e\n",
      "                           split | train\n",
      "                               n | 50000\n",
      "                          source | HuggingFace\n",
      "             source_dataset_name | wmt14\n",
      "                           stage | filtered\n",
      "                         created | 2025-08-27 13:00:47.317834\n",
      "                            lang | fr-en\n",
      "                        lang_src | en\n",
      "                        lang_tgt | fr\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2895/50000 [00:00<00:00, 54613.58it/s]\n",
      "[08/27/2025 01:00:54 PM] [INFO] [mini_transformer.data.repo] [add] : Added dataset id: a768da37, name: wmt14-fr-en-validation-filtered-50000_examples-a768da37 to the Dataset repository.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                       TranslationDataset                       \n",
      "                              id | a768da37\n",
      "                            name | wmt14-fr-en-validation-filtered-50000_examples-a768da37\n",
      "                           split | validation\n",
      "                               n | 2895\n",
      "                          source | HuggingFace\n",
      "             source_dataset_name | wmt14\n",
      "                           stage | filtered\n",
      "                         created | 2025-08-27 13:00:54.263693\n",
      "                            lang | fr-en\n",
      "                        lang_src | en\n",
      "                        lang_tgt | fr\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2968/50000 [00:00<00:00, 55077.11it/s]\n",
      "[08/27/2025 01:00:54 PM] [INFO] [mini_transformer.data.repo] [add] : Added dataset id: bf240e52, name: wmt14-fr-en-test-filtered-50000_examples-bf240e52 to the Dataset repository.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                       TranslationDataset                       \n",
      "                              id | bf240e52\n",
      "                            name | wmt14-fr-en-test-filtered-50000_examples-bf240e52\n",
      "                           split | test\n",
      "                               n | 2968\n",
      "                          source | HuggingFace\n",
      "             source_dataset_name | wmt14\n",
      "                           stage | filtered\n",
      "                         created | 2025-08-27 13:00:54.383958\n",
      "                            lang | fr-en\n",
      "                        lang_src | en\n",
      "                        lang_tgt | fr\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = repo.show(stage=\"raw\")\n",
    "for dataset_id in datasets['id']:\n",
    "    dataset_filter(dataset_id=dataset_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minix4mr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
